{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep data for error bars\n",
    "\n",
    "### The function below is what you will change to shift to confidence intervals. You should be able to replace things like per10/per20/... with whatever manipulation you want.\n",
    "\n",
    "## If you change shaded_error_prep(), single_shaded_error_fig() will need to be updated to support the changes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes in df of a subject's trials, then performs calculations on them and outputs the new df\n",
    "\n",
    "def shaded_error_prep(hold):\n",
    "    hold = hold.reindex(sorted(hold.columns), axis = 1)\n",
    "\n",
    "    mean = hold.mean(axis=1,skipna=True)\n",
    "\n",
    "    median = pd.DataFrame(hold.median(axis=1,skipna=True))\n",
    "\n",
    "    ## Percentiles\n",
    "\n",
    "    upper_percentile = pd.DataFrame(np.nanpercentile(hold, 75, axis = 1))\n",
    "    mid_percentile = pd.DataFrame(np.nanpercentile(hold, 50, axis = 1))\n",
    "    low_percentile = pd.DataFrame(np.nanpercentile(hold, 25, axis = 1))\n",
    "\n",
    "    per10 = pd.DataFrame(np.nanpercentile(hold, 10, axis = 1))\n",
    "    per20 = pd.DataFrame(np.nanpercentile(hold, 20, axis = 1))\n",
    "    per30 = pd.DataFrame(np.nanpercentile(hold, 30, axis = 1))\n",
    "    per40 = pd.DataFrame(np.nanpercentile(hold, 40, axis = 1))\n",
    "\n",
    "    min = hold.min(axis=1)\n",
    "\n",
    "    max = hold.max(axis=1)\n",
    "\n",
    "    std = hold.std(axis=1, skipna = True,)\n",
    "\n",
    "    hold['10%'] = per10\n",
    "    hold['20%'] = per20\n",
    "    hold['30%'] = per30\n",
    "    hold['40%'] = per40\n",
    "\n",
    "    hold['mean'] = mean\n",
    "    hold['75%'] = upper_percentile\n",
    "    hold['25%'] = low_percentile\n",
    "    hold['median'] = median\n",
    "\n",
    "    hold['min'] = min\n",
    "    hold['max'] = max \n",
    "    hold['std'] = std\n",
    "\n",
    "    return hold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is responsible for printing the figures\n",
    "\n",
    "### acc_shaded_error produces multi-fig displays, like the pre-post or sess2&5 graphs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go \n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "\n",
    "## Inputs:\n",
    "# folder: output file folder path (where you want to save figs to)\n",
    "# name: what you would like to call this particular file\n",
    "# df_list: list of dfs you would like to plot. each df will receive its own subplot on the figure. (can be swapped between vertical and horizontal arrangement by flipping i and 1)\n",
    "def acc_shaded_error(folder, name, df_list):\n",
    "    rows = len(df_list)\n",
    "    fig = make_subplots(rows=rows, cols=1)\n",
    "\n",
    "    i = 0\n",
    "    for df in df_list:\n",
    "        i += 1\n",
    "        fig = single_shaded_error_fig(fig,df,i, 1)\n",
    "\n",
    "    save_path = 'chunk_accs_figs/' + folder + '/' + name + '.png'\n",
    "\n",
    "    fig.write_image(save_path)\n",
    "\n",
    "\n",
    "## Inputs:\n",
    "# Should basically be handled by previous function\n",
    "# Fig: fig object to add plot to\n",
    "# df: df to use for plot\n",
    "# row: row position in figure\n",
    "# col: col position in figure\n",
    "#\n",
    "# Any edits made in shaded_error_prep must be cascaded into this function\n",
    "def single_shaded_error_fig(fig, df, row, col):\n",
    "\n",
    "    legend = True\n",
    "    if row*col > 1:\n",
    "        legend = False\n",
    "\n",
    "    x_vals = list(df.index)\n",
    "    y_vals = df['median']\n",
    "    y_10 = df['10%']\n",
    "    y_20 = df['20%']\n",
    "    y_30 = df['30%']\n",
    "    y_40 = df['40%']\n",
    "    y_75 = df['75%']\n",
    "\n",
    "\n",
    "    fig.add_scatter(\n",
    "        name = '10%',\n",
    "        x = x_vals,\n",
    "        y = y_10,\n",
    "        mode = 'lines',\n",
    "        marker = dict(color=\"#33ccff\"),\n",
    "        line = dict(width=1.5),\n",
    "        showlegend=legend,\n",
    "        row = row,\n",
    "        col = col\n",
    "    )\n",
    "    fig.add_scatter(\n",
    "        name = '20%',\n",
    "        x = x_vals,\n",
    "        y = y_20,\n",
    "        mode = 'lines',\n",
    "        marker = dict(color=\"#03fce3\"),\n",
    "        line = dict(width=1.5),\n",
    "        fillcolor = 'rgba(68, 68, 68, 0.3)',\n",
    "        fill='tonexty',\n",
    "        showlegend=legend,\n",
    "        row = row,\n",
    "        col = col\n",
    "    )\n",
    "    fig.add_scatter(\n",
    "        name = '30%',\n",
    "        x = x_vals,\n",
    "        y = y_30,\n",
    "        mode = 'lines',\n",
    "        marker = dict(color=\"#9dfc03\"),\n",
    "        line = dict(width=1.5),\n",
    "        fillcolor = 'rgba(68, 68, 68, 0.3)',\n",
    "        fill='tonexty',\n",
    "        showlegend=legend,\n",
    "        row = row,\n",
    "        col = col\n",
    "    )\n",
    "    fig.add_scatter(\n",
    "        name = '40%',\n",
    "        x = x_vals,\n",
    "        y = y_40,\n",
    "        mode = 'lines',\n",
    "        marker = dict(color=\"#ffd966\"),\n",
    "        line = dict(width=1.5),\n",
    "        fillcolor = 'rgba(68, 68, 68, 0.3)',\n",
    "        fill='tonexty',\n",
    "        showlegend=legend,\n",
    "        row = row,\n",
    "        col = col\n",
    "    )\n",
    "    fig.add_scatter(\n",
    "        name = 'median',\n",
    "        x = x_vals,\n",
    "        y = y_vals,\n",
    "        mode = 'lines',\n",
    "        marker = dict(color=\"#ff6666\"),\n",
    "        line = dict(width=1.5),\n",
    "        fillcolor = 'rgba(68, 68, 68, 0.3)',\n",
    "        fill='tonexty',\n",
    "        showlegend=legend,\n",
    "        row = row,\n",
    "        col = col\n",
    "    )\n",
    "    fig.add_scatter(\n",
    "        name = '75%',\n",
    "        x = x_vals,\n",
    "        y = y_75,\n",
    "        mode = 'lines',\n",
    "        marker = dict(color=\"#cc33ff\"),\n",
    "        line = dict(width=1.5),\n",
    "        showlegend=legend,\n",
    "        fillcolor = 'rgba(68, 68, 68, 0.3)',\n",
    "        fill='tonexty',\n",
    "        row = row,\n",
    "        col = col\n",
    "    )    \n",
    "    \n",
    "    fig.update_layout(\n",
    "        yaxis_title='rolling acc (25ticks)',\n",
    "        title=name,\n",
    "        hovermode=\"x\"\n",
    "    )\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This bit makes figures for single participant single session. just a way for you to see the general structure of this figure-maker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob(\"chunk_accs/subj_accs/*.csv\")\n",
    "\n",
    "# Gets some relevant characteristics out of known file path structures\n",
    "def get_subjsess(path):\n",
    "    file = path.split('/')[-1]\n",
    "\n",
    "    hold = file.split('_')\n",
    "\n",
    "    subj = hold[0]\n",
    "    sess = hold[1]\n",
    "\n",
    "    return subj,sess\n",
    "\n",
    "for path in files:\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    subj, sess = get_subjsess(path)\n",
    "\n",
    "    name = subj + '_' + sess\n",
    "\n",
    "    print(name)\n",
    "\n",
    "    acc_shaded_error('subj_accs_figs', name,[df])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This next bit preps the files for use in figures\n",
    "\n",
    "### intake files should be included in this folder, check subj_accs\n",
    "\n",
    "### This next chunk takes individual files and builds different aggregated files, for groupings like bio/arb, motion class, and session. Feel free to group however you like, and use column names in subJ_accs to build new dfs.\n",
    "\n",
    "#### I just aggregated things by different groupings so that it would be easier to process later. you can prep however you like really, as long as your data is in the right format by the time it goes into acc_shaded_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bio_files = glob.glob(\"chunk_accs/subj_accs/*bi*.csv\")\n",
    "arb_files = glob.glob(\"chunk_accs/subj_accs/*ar*.csv\")\n",
    "\n",
    "## Remove problem participants\n",
    "\n",
    "bi_rest = pd.DataFrame()\n",
    "bi_open = pd.DataFrame()\n",
    "bi_close = pd.DataFrame()\n",
    "ar_rest = pd.DataFrame()\n",
    "ar_open = pd.DataFrame()\n",
    "ar_close = pd.DataFrame()\n",
    "\n",
    "sess2_files = [file for file in files if 'sess2' in file]\n",
    "sess5_files = [file for file in files if 'sess5' in file]\n",
    "\n",
    "which_files = [(sess2_files, 'sess2'),(sess5_files, 'sess5')]\n",
    "\n",
    "for pair in which_files:\n",
    "    files = pair[0]\n",
    "    session = pair[1]\n",
    "    for path in files:\n",
    "        subj,sess = get_subjsess(path)\n",
    "        name = subj + '_' + sess + '_'\n",
    "        df = pd.read_csv(path)\n",
    "        rest = 0\n",
    "        open = 0\n",
    "        close = 0\n",
    "        if 'sub-bi' in path:\n",
    "            for column in df.columns.values:\n",
    "                if 'rest' in column:\n",
    "                    rest += 1\n",
    "                    bi_rest[name + column] = df[column]\n",
    "                elif 'open' in column:\n",
    "                    open += 1\n",
    "                    bi_open[name + column] = df[column]\n",
    "                elif 'close' in column:\n",
    "                    close += 1\n",
    "                    bi_close[name + column] = df[column]\n",
    "                    \n",
    "        elif 'sub-ar' in path:\n",
    "            for column in df.columns.values:\n",
    "                if 'rest' in column:\n",
    "                    rest += 1\n",
    "                    ar_rest[name + column] = df[column]\n",
    "                elif 'open' in column:\n",
    "                    open += 1\n",
    "                    ar_open[name + column] = df[column]\n",
    "                elif 'close' in column:\n",
    "                    close += 1\n",
    "                    ar_close[name + column] = df[column]\n",
    "                    \n",
    "    save_path = 'chunk_accs/aggregate_accs/' + session + '_'\n",
    "\n",
    "    def reindex_df(df):\n",
    "        return df.reindex(sorted(df.columns), axis=1)\n",
    "\n",
    "    bi_agg = pd.DataFrame()\n",
    "    bi_agg = bi_rest\n",
    "    bi_agg = bi_agg.join(bi_open)\n",
    "    bi_agg = bi_agg.join(bi_close)\n",
    "\n",
    "    ar_agg = pd.DataFrame()\n",
    "    ar_agg = ar_rest\n",
    "    ar_agg = ar_agg.join(ar_open)\n",
    "    ar_agg = ar_agg.join(ar_close)\n",
    "\n",
    "\n",
    "    bi_agg = shaded_error_prep(reindex_df(bi_agg))\n",
    "    ar_agg = shaded_error_prep(reindex_df(ar_agg))\n",
    "\n",
    "\n",
    "    bi_rest = shaded_error_prep(reindex_df(bi_rest))\n",
    "    bi_open = shaded_error_prep(reindex_df(bi_open))\n",
    "    bi_close = shaded_error_prep(reindex_df(bi_close))\n",
    "    ar_rest = shaded_error_prep(reindex_df(ar_rest))\n",
    "    ar_open = shaded_error_prep(reindex_df(ar_open))\n",
    "    ar_close = shaded_error_prep(reindex_df(ar_close))\n",
    "\n",
    "\n",
    "    bi_agg.to_csv(save_path + 'bio_agg.csv')\n",
    "    bi_rest.to_csv(save_path + 'bio_rest_aggregate.csv')\n",
    "    bi_open.to_csv(save_path + 'bio_open_aggregate.csv') \n",
    "    bi_close.to_csv(save_path + 'bio_close_aggregate.csv')\n",
    "    ar_agg.to_csv(save_path + 'arb_agg.csv')\n",
    "    ar_rest.to_csv(save_path + 'arb_rest_aggregate.csv')\n",
    "    ar_open.to_csv(save_path + 'arb_open_aggregate.csv')\n",
    "    ar_close.to_csv(save_path + 'arb_close_aggregate.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Builds Error Bars for sess2 & 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_files = glob.glob(\"chunk_accs/aggregate_accs/*.csv\")\n",
    "\n",
    "for file in agg_files:\n",
    "    \n",
    "    hold = pd.read_csv(file)\n",
    "\n",
    "    bits = file.split('/')[-1]\n",
    "    bits = bits.split('_')\n",
    "    sess = bits[0]\n",
    "    group = bits[1]\n",
    "    gesture = bits[2]\n",
    "\n",
    "\n",
    "    df_list = [hold]\n",
    "    if 'sess2' in file:\n",
    "        file2 = list(filter(lambda a: 'sess5_' + group + '_' + gesture in a, agg_files))[0]\n",
    "        if file2:\n",
    "            df_list.append(pd.read_csv(file2))\n",
    "\n",
    "            sess = 'sess2&5'\n",
    "    elif 'sess5' in file:\n",
    "        continue \n",
    "\n",
    "    name = ''\n",
    "    if 'agg.csv' in file:\n",
    "        name = group + '_' + sess \n",
    "    else:\n",
    "        name = group + '_' + gesture + '_' + sess\n",
    "\n",
    "    acc_shaded_error('paired_agg_accs_figs', name, df_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Builds same error bars but for pre & post within a single session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_post_agg_files = glob.glob(\"chunk_accs/agg_pre_post_accs/*.csv\")\n",
    "\n",
    "for file in pre_post_agg_files:\n",
    "    \n",
    "    hold = pd.read_csv(file)\n",
    "\n",
    "    bits = file.split('/')[-1]\n",
    "    print(bits)\n",
    "    bits = bits.split('_')\n",
    "    group = bits[0]\n",
    "    gesture = bits[1]\n",
    "    pre_post = bits[2]\n",
    "\n",
    "\n",
    "    df_list = [hold]\n",
    "    if 'pre' in bits:\n",
    "        file2 = list(filter(lambda a: group + '_' + gesture + '_post' in a, pre_post_agg_files))[0]\n",
    "        if file2:\n",
    "            print('Appending... ', file2)\n",
    "            df_list.append(pd.read_csv(file2))\n",
    "\n",
    "            pre_post = 'pre_post'\n",
    "    elif 'post' in file:\n",
    "        continue \n",
    "\n",
    "    \n",
    "    name = group + '_' + gesture + '_' + pre_post\n",
    "\n",
    "    acc_shaded_error('agg_prepost_accs_figs', name, df_list)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
